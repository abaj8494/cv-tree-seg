\section{Discussion}
\subsection{Overview}
It is rather obvious that deep learning algorithms fared much better at this task than the more basic machine learning algorithms such as Random Forests, or Stochastic gradient. The masks generated by the UNET algorithm are good enough that some postprocessing (closing the image or opening it) significantly improve it's IoU .
\subsection{Random Forests}
\includegraphics[width=.95\linewidth]{figs/RFmask.jpg}\\
When we look at the masks produced by the random forest, we can see that part of the reason for the poor performance is due to it reacting to spots where there are no trees, and due to the mask being far too small at spots where there is a dead tree, but the mask did detect almost all of the dead tree spots.  Likely what is happening is the RF determines each pixel to be a dead tree based on its colour, which would result in this behaviour. This is probably due to the random forest implementation being independent of location (the pixel information was passed to the random forest with no information of what it was nearby). this would likely be improved by the kernel based RF implementation, as described in the methods section.
\subsection{SGD}
\includegraphics[width=.95\linewidth]{figs/SGDmask.jpg}\\
Upon looking at the prediction mask produced by the SGD classifier, it can immediately be seen that there is a striking contrast compared to the random forest classifier. There is a much higher recall rate with the SGD, however the false positive has also skyrocketed as the model is mis-classifying many of the background pixels as dead tree pixels (foreground).\\
There could be several explanations for this phenomenon; most notably, the dataset is quite imbalanced, the input images mostly consist of background pixels and the model could be overcompensating for the class imbalance by being bias towards making false positives.\\
Indeed, the model does actually perform somewhat better on images that have a higher concentration of dead tree pixels compared to images that have very few. Secondly, using only RGB/NRG pixel values is weak as it ignores spatial context and information, combining this with class imbalance makes it more difficult to differentiate between foreground and background pixels.\\
While the results are not very promising, there are considerable avenues in which the SGD model could be optimised, firstly a stricter threshold for predicting foreground pixels would likely reduce the false positive rate, extra features that capture local spatial information or texture (LBP/HoG) could help with more accurately differentiating foreground/background pixels.\\
The SGD model also trains considerably faster than the other models, able to reach convergence in seconds while the random forest model took significantly longer.
%\includegraphics[width=.95\linewidth]{figs/sgd-training.png}\\
\begin{lstlisting}[caption={SGD Training}, label={fig:sgd-train}]
-- Epoch 1
Norm: 1.49, NNZs: 6, Bias: -0.716938, T: 5455872, Avg. loss: 0.870819
Total training time: 0.62 seconds.
-- Epoch 2
Norm: 1.42, NNZs: 6, Bias: -0.554052, T: 10911744, Avg. loss: 0.572698
Total training time: 1.25 seconds.
-- Epoch 3
Norm: 1.53, NNZs: 6, Bias: -0.679788, T: 16367616, Avg. loss: 0.564680
Total training time: 1.89 seconds.
-- Epoch 4
Norm: 1.57, NNZs: 6, Bias: -0.654369, T: 21823488, Avg. loss: 0.561824
Total training time: 2.53 seconds.
-- Epoch 5
Norm: 1.50, NNZs: 6, Bias: -0.730205, T: 27279360, Avg. loss: 0.558549
Total training time: 3.17 seconds.
-- Epoch 6
Norm: 1.60, NNZs: 6, Bias: -0.806963, T: 32735232, Avg. loss: 0.557498
Total training time: 3.80 seconds.
-- Epoch 7
Norm: 1.61, NNZs: 6, Bias: -0.725729, T: 38191104, Avg. loss: 0.555748
Total training time: 4.44 seconds.
-- Epoch 8
Norm: 1.61, NNZs: 6, Bias: -0.692419, T: 43646976, Avg. loss: 0.556135
Total training time: 5.07 seconds.
-- Epoch 9
Norm: 1.60, NNZs: 6, Bias: -0.616872, T: 49102848, Avg. loss: 0.554584
Total training time: 5.71 seconds.
-- Epoch 10
Norm: 1.60, NNZs: 6, Bias: -0.598181, T: 54558720, Avg. loss: 0.554805
Total training time: 6.35 seconds.
-- Epoch 11
Norm: 1.62, NNZs: 6, Bias: -0.689664, T: 60014592, Avg. loss: 0.554993
Total training time: 6.98 seconds.
-- Epoch 12
Norm: 1.62, NNZs: 6, Bias: -0.618751, T: 65470464, Avg. loss: 0.553864
Total training time: 7.61 seconds.
-- Epoch 13
Norm: 1.59, NNZs: 6, Bias: -0.656321, T: 70926336, Avg. loss: 0.554055
Total training time: 8.25 seconds.
-- Epoch 14
Norm: 1.65, NNZs: 6, Bias: -0.698402, T: 76382208, Avg. loss: 0.555047
Total training time: 8.88 seconds.
Convergence after 14 epochs took 8.88 seconds
\end{lstlisting}

\subsection{SVM}
Unfortunately, we were unable to train a SVM classifier given that the small feature space relative to the size of the dataset. Recall that each training sample is a 6-dimensional vector that corresponded to a dead tree pixel or a background pixel. Given the size of the training data and the fact the classes were quite imbalanced, it was not possible to find a hyperplane that reasonably separated pixels into the 2 categories.\\


\subsection{CNN}
To address class imbalance and enhance segmentation quality, a loss function combines binary Cross-Entropy and Dice Loss. The adam optimizer effectively minimized the hybrid loss through adaptive learning rate adjustment. Compared to the results, the simple CNN model achieved higher precision than the encoder-decoder model, which IoU are 0.28 and 0.16 separately. However, the encoder-decoder model has significantly faster training speeds and testing times. Notably, RGB inputs consistently outperformed NIR-only data across both models, though the RGB+NIR combination provided marginal improvements over RGB alone in the simple CNN. These findings suggest a fundamental trade-off between segmentation accuracy and computational efficiency in this method. 
\begin{lstlisting}[language=Python]
def dice_loss(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

def combo_loss(y_true, y_pred):
    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
    return bce + dice_loss(y_true, y_pred)
\end{lstlisting}
\includegraphics[width = 9cm]{figs/CNN_mask.jpg}\\
\includegraphics[width = 9cm]{figs/CNN_mask2.jpg}\\
\includegraphics[width = 9cm]{figs/CNN_mask3.jpg}\\
% that's better jing jing, do you mind adding captions so it doesn't look like 1 huge image?
\subsection{CBAM}
\includegraphics[width=1\linewidth]{figs/DeepLabExample.png}
\paragraph{Backbone depth.}
Replacing ResNet‐18 with deeper variants (ResNet‐50 and ResNet‐101) does \emph{not}
translate into higher accuracy: mIoU changes by \(+0.2\) pp and \(-0.1\) pp,
respectively, which is within statistical noise.
The likely reason is that the training set (444 images) is too small for parameters to be learnt properly; they converge to a poor local minimum in which both training and validation predictions are nearly constant.

\paragraph{Augmentation intensity.}
A controlled sweep over augmentation strength shows a bell-shaped trend:
\emph{mild} transforms (flip, \(\pm30^\circ\) rotation) add \(+\!1.3\) mIoU, but \emph{aggressive} settings (\(\pm45^\circ\) rotation, Cutout 40 \%, Gaussian blur \(\sigma{=}3\)) drop performance by \(3.7\) pp.
Excessive geometric distortion breaks the thin‐structure prior that the
network needs to recognise dead branches, while heavy photometric jitter
destroys the cross-spectral cues learned from the NIR and visible bands.

\input{05.5-aj-unet}